{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN+jYf4lkx+Iwh/8SBGI4Bm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jaydengonzal2021/Statistics/blob/main/JaydenGonzalezMidterm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q53CjDtnQzwU"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X = np.array([12,9,20,7,2])    # <- Put your numbers here  Diameter\n",
        "Y = np.array([38,28,63,22,6])  #   Circumference"
      ],
      "metadata": {
        "id": "QLYjkKiMQ6ww"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(X,Y,'g.') #This made a plot where the x axis is diameter and y axis is circumference"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "6bg-X15rRZFQ",
        "outputId": "0f1db498-86ef-479a-fbb2-fbacff496bbf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7fdc07166710>]"
            ]
          },
          "metadata": {},
          "execution_count": 32
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "m_guess = 0 #setting guesses at 0 for now\n",
        "b_guess = 0"
      ],
      "metadata": {
        "id": "nz5DnZQ5RcPe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "y = mx + b"
      ],
      "metadata": {
        "id": "NN2poXgfS3vQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "m_guess * X + b_guess #this is the equation to make a y guess  all the numbers are zero now cuz the m and b guesses are currently set at 0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mTs9Kxp8SKe-",
        "outputId": "05acb184-10bc-482b-c1fa-ffc68e04aa91"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 0, 0, 0, 0])"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_guess = m_guess * X + b_guess #defined the y_guess variable"
      ],
      "metadata": {
        "id": "5cUYKymrSOOv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_guess - Y #subtract the real values from the guess in this case it subtracted all from zero"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8gbHlTDyTUHx",
        "outputId": "948e25a8-fa5d-490b-c3e3-b452f07a4b2c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-38, -28, -63, -22,  -6])"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "(y_guess - Y)**2 #he does this because the numbers were all negative and now they are positive and still proportional"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qk6Xz-yfTszS",
        "outputId": "0d8cbd5f-40db-43a5-93a8-6672e92339e1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1444,  784, 3969,  484,   36])"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.sum((y_guess - Y)**2) #this is the loss equation"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PzznmmYEUUFz",
        "outputId": "859f73d9-e473-4df8-8967-14728a78b6d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "6717"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss = np.sum((y_guess - Y)**2) #defining the loss variable"
      ],
      "metadata": {
        "id": "vb6QRC39U9Vi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for m_guess in np.linspace (-100,100,100): #this will search from -100 to 100 with 100 steps on the way (so every 2ish points) then telling us each guess\n",
        "  print(m_guess)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kFvN_t-fWa1y",
        "outputId": "cad8565f-dc1e-477f-af57-6b77110f592b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-100.0\n",
            "-97.97979797979798\n",
            "-95.95959595959596\n",
            "-93.93939393939394\n",
            "-91.91919191919192\n",
            "-89.8989898989899\n",
            "-87.87878787878788\n",
            "-85.85858585858585\n",
            "-83.83838383838383\n",
            "-81.81818181818181\n",
            "-79.79797979797979\n",
            "-77.77777777777777\n",
            "-75.75757575757575\n",
            "-73.73737373737373\n",
            "-71.71717171717171\n",
            "-69.69696969696969\n",
            "-67.67676767676767\n",
            "-65.65656565656565\n",
            "-63.63636363636363\n",
            "-61.61616161616161\n",
            "-59.59595959595959\n",
            "-57.57575757575757\n",
            "-55.55555555555555\n",
            "-53.53535353535353\n",
            "-51.515151515151516\n",
            "-49.494949494949495\n",
            "-47.474747474747474\n",
            "-45.45454545454545\n",
            "-43.43434343434343\n",
            "-41.41414141414141\n",
            "-39.39393939393939\n",
            "-37.37373737373737\n",
            "-35.35353535353535\n",
            "-33.33333333333333\n",
            "-31.313131313131308\n",
            "-29.292929292929287\n",
            "-27.272727272727266\n",
            "-25.252525252525245\n",
            "-23.232323232323225\n",
            "-21.212121212121204\n",
            "-19.191919191919183\n",
            "-17.171717171717162\n",
            "-15.151515151515142\n",
            "-13.13131313131312\n",
            "-11.1111111111111\n",
            "-9.09090909090908\n",
            "-7.0707070707070585\n",
            "-5.050505050505038\n",
            "-3.030303030303031\n",
            "-1.0101010101010104\n",
            "1.0101010101010104\n",
            "3.030303030303031\n",
            "5.050505050505052\n",
            "7.070707070707073\n",
            "9.090909090909093\n",
            "11.111111111111114\n",
            "13.131313131313135\n",
            "15.151515151515156\n",
            "17.171717171717177\n",
            "19.191919191919197\n",
            "21.212121212121218\n",
            "23.23232323232324\n",
            "25.25252525252526\n",
            "27.27272727272728\n",
            "29.2929292929293\n",
            "31.313131313131322\n",
            "33.33333333333334\n",
            "35.353535353535364\n",
            "37.373737373737384\n",
            "39.393939393939405\n",
            "41.414141414141426\n",
            "43.43434343434345\n",
            "45.45454545454547\n",
            "47.47474747474749\n",
            "49.49494949494951\n",
            "51.51515151515153\n",
            "53.53535353535355\n",
            "55.55555555555557\n",
            "57.57575757575759\n",
            "59.59595959595961\n",
            "61.616161616161634\n",
            "63.636363636363654\n",
            "65.65656565656568\n",
            "67.6767676767677\n",
            "69.69696969696972\n",
            "71.71717171717174\n",
            "73.73737373737376\n",
            "75.75757575757578\n",
            "77.7777777777778\n",
            "79.79797979797982\n",
            "81.81818181818184\n",
            "83.83838383838386\n",
            "85.85858585858588\n",
            "87.8787878787879\n",
            "89.89898989898992\n",
            "91.91919191919195\n",
            "93.93939393939394\n",
            "95.95959595959596\n",
            "97.97979797979798\n",
            "100.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "best_loss_so_far = 10000000000000 #this just sets a huge number that the computer can compare against the found loss values to try and find a lower one\n",
        "m_best = 0\n",
        "b_best = 0 #these lines set an ideal loss of 0\n",
        "for m_guess in np.linspace (-100,100,100):\n",
        "  for b_guess in np.linspace(-100,100,100): #so this is putting in every value of m and b, then we will use this data with the loss equation to find the line of best fit\n",
        "  \n",
        "    y_guess = m_guess * X + b_guess #this is just defining the y guess as this new equation\n",
        "\n",
        "    loss = np.sum((y_guess - Y)**2) #now this block will also find the loss as well as make the m and b guesses (it finds loss using the y)\n",
        "\n",
        "    if loss < best_loss_so_far:\n",
        "      m_best = m_guess\n",
        "      b_best = b_guess\n",
        "      best_loss_so_far = loss #these 4 lines will compare the loss tests to the best known loss and then either replace it (and the guesses that found it) if its better or ignore it if it is worse\n",
        "     \n",
        "print(\"found best fit:\") #this will create a line of text when it finds a new best loss\n",
        "print(m_best,b_best,loss) #this will print the loss value next to the guess so you can actually see what tests have what loss value (a low loss value represents a good line of best fit)(it will print m guess then b guess than the loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YPDv22P_Wt77",
        "outputId": "d848b2e7-af9f-4632-bd74-81f48fdbc7c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "found best fit:\n",
            "3.030303030303031 1.0101010101010104 7378517.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LRc0xgVqXPjn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Question 1:\n",
        "\n",
        "What do $m$ and $b$ represent in these equations?\n"
      ],
      "metadata": {
        "id": "fYgQ5VJ00Y2Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "the M and B represent the same M and B from the slope equation (y=mx+b). Specifically M represents the slope of the line and B represents the y-intercept of the line."
      ],
      "metadata": {
        "id": "__L4DKLE0hjt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Question 2:\n",
        "\n",
        "Guess what the correct values $m$ and $b$ should be."
      ],
      "metadata": {
        "id": "64pa-lCN0jwf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "There is no need to guess because the code found the correct values for minimum loss. You can see above the correct M value is 3.03... and the correct b value is 1.01..."
      ],
      "metadata": {
        "id": "fmM_ogTQ0j3Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Question 3:\n",
        "\n",
        "Does it make sense to include $b$ in the parameters?"
      ],
      "metadata": {
        "id": "hYaGlB_w0kB3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "It does make sense to include B, or the y-intercept, in the parameters. This is because we are trying to find the line of best fit and in order to best fit the points on the graph it needs to start out at the proper y-intercept. For example you could have the right slope, or M, but if the line started at y=200 then you'd miss all the points on the graph."
      ],
      "metadata": {
        "id": "SiMzmbRT0kGe"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZBuWLBBd2bGu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}